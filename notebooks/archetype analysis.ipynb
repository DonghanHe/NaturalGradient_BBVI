{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianAA(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_archetypes,\n",
    "                 num_data,\n",
    "                 dataset,\n",
    "                 data_archetype_prior = 1.,\n",
    "                 archetype_data_prior = 1.,\n",
    "                 natgrad = False,\n",
    "                 loss = 'l2'\n",
    "                 ):\n",
    "        super(BayesianAA, self).__init__()\n",
    "        self.T = num_archetypes\n",
    "        self.N = num_data\n",
    "        self.alpha = data_archetype_prior\n",
    "        self.eta = archetype_data_prior\n",
    "        self.natgrad = natgrad\n",
    "        self.data = dataset # should be of shape (D, F)\n",
    "\n",
    "        self.z_prior = td.Dirichlet(torch.ones((self.T,)) * self.alpha)\n",
    "        self.t_prior = td.Dirichlet(torch.ones((self.N,)) * self.eta)\n",
    "        self.t_logits = torch.nn.Parameter(torch.FloatTensor(self.T, self.N, ))\n",
    "        torch.nn.init.xavier_uniform_(self.t_logits, )\n",
    "\n",
    "        if loss == 'l2':\n",
    "            self.loss = self.l2loss2\n",
    "        elif loss == 'bce':\n",
    "            self.loss = self.bceloss\n",
    "\n",
    "    @staticmethod\n",
    "    def l2loss(x_mu, x):\n",
    "        return torch.sum(-0.5*(x-x_mu)**2 - 0.5 * torch.log(torch.tensor(3.1415926*2)), dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def l2loss2(x_mu, x):\n",
    "        return -torch.nn.MSELoss(reduction='sum')(x_mu, x)/2\n",
    "\n",
    "    @staticmethod\n",
    "    def bceloss(x_mu, x):\n",
    "        return -torch.nn.BCELoss(reduction='sum')(x_mu, x)\n",
    "    \n",
    "    def forward(self, x, z_logits):\n",
    "        z_params = torch.clamp(torch.nn.Softplus()(z_logits), min=1e-3, max=1e3)\n",
    "        if self.natgrad:\n",
    "            z_post = td.Dirichlet(dirichlet_natgrad_backprop.apply(z_params), validate_args=False)\n",
    "        else:\n",
    "            z_post = td.Dirichlet(z_params, validate_args=False)\n",
    "        z_sample = z_post.rsample()\n",
    "        \n",
    "        t_params = torch.clamp(torch.nn.Softplus()(self.t_logits), min = 1e-3, max=1e3)\n",
    "        if self.natgrad:\n",
    "            t_post = td.Dirichlet(dirichlet_natgrad_backprop.apply(t_params), validate_args=False)\n",
    "        else:\n",
    "            t_post = td.Dirichlet(t_params, validate_args=False)\n",
    "        t_sample = t_post.rsample()\n",
    "\n",
    "        x_hat = torch.matmul(z_sample, torch.matmul(t_sample, self.data))\n",
    "        logp = self.loss(x_hat, x)\n",
    "\n",
    "        reconstruct_loss = logp / z_logits.shape[0]\n",
    "        with torch.no_grad():\n",
    "            kl_t = torch.sum(self.klqp(t_post, self.t_prior))  # of shape (T, 1) before taking sum\n",
    "            kl_z = torch.mean(self.klqp(z_post, self.z_prior))  # of shape (D_batch, 1) before taking mean\n",
    "\n",
    "        return reconstruct_loss * self.N, kl_z * self.N + kl_t\n",
    "    \n",
    "    def sample(self, z_logits, mean = False):\n",
    "        z_params = torch.clamp(torch.nn.Softplus()(z_logits), min=1e-3, max=1e3)\n",
    "        z_post = td.Dirichlet(z_params, validate_args=False)\n",
    "        if mean == False:\n",
    "            z_sample = z_post.rsample()\n",
    "        else:\n",
    "            z_sample = z_post.mean\n",
    "\n",
    "        t_params = torch.clamp(torch.nn.Softplus()(self.t_logits), min=.1, max=1e3)\n",
    "        t_post = td.Dirichlet(t_params, validate_args=False)\n",
    "        if mean == False:\n",
    "            t_sample = t_post.rsample()\n",
    "        else:\n",
    "            t_sample = t_post.mean\n",
    "\n",
    "        x_hat = torch.matmul(z_sample, torch.matmul(t_sample, self.data))\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def klqp(self, approx_posterior, prior):\n",
    "        return td.kl_divergence(approx_posterior, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dirichlet_natgrad_backprop(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        # forward propagation is just the identity\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dinputs):\n",
    "        inputs, = ctx.saved_tensors\n",
    "        dinputs_tilde = dirichlet_multiply_by_fisher_inv(inputs, dinputs)\n",
    "        return dinputs_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_bp(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs0, inputs1):\n",
    "        ctx.save_for_backward(inputs0, inputs1)\n",
    "        return inputs0\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dinputs):\n",
    "        inputs0, inputs1, =ctx.saved_tensors\n",
    "        dinputs_tilde = 5.*inputs0+inputs1\n",
    "        print(dinputs_tilde)\n",
    "        return dinputs_tilde, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.1210,  4.7840,  3.5340,  0.3473],\n        [ 3.1123, -0.9900, -3.0418,  0.5660],\n        [-4.9860, -0.4865, -3.6989, -4.1422]])\n"
     ]
    }
   ],
   "source": [
    "test_parameters = torch.nn.Parameter(torch.FloatTensor(3,4),requires_grad=True)\n",
    "torch.nn.init.kaiming_uniform_(test_parameters)\n",
    "\n",
    "res = torch.sum(custom_bp.apply(test_parameters, torch.ones((4,), requires_grad=False)))\n",
    "\n",
    "res.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.5762, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\ntensor([[ 0.6242,  0.7568,  0.5068, -0.1305],\n        [ 0.4225, -0.3980, -0.8084, -0.0868],\n        [-1.1972, -0.2973, -0.9398, -1.0284]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
